# íŒê³¼ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

## ê°œìš”

CAI ê°œë°œ ì¤‘ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²° ë°©ë²•, ê·¸ë¦¬ê³  ì„±ëŠ¥ ìµœì í™” íŒì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## í™˜ê²½ ê´€ë ¨ ë¬¸ì œ

### MPS (Metal Performance Shaders) ì˜¤ë¥˜

#### ì¦ìƒ
```
RuntimeError: MPS backend out of memory
```

#### í•´ê²° ë°©ë²•
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
BATCH_SIZE = 16  # 32 â†’ 16

# ë˜ëŠ” Fallback í™œì„±í™”
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

### MPSì—ì„œ íŠ¹ì • ì—°ì‚° ë¯¸ì§€ì›

#### ì¦ìƒ
```
NotImplementedError: The operator 'aten::...' is not currently implemented for the MPS device
```

#### í•´ê²° ë°©ë²•
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í„°ë¯¸ë„ì—ì„œ)
export PYTORCH_ENABLE_MPS_FALLBACK=1

# ë˜ëŠ” Python ì½”ë“œì—ì„œ
import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
```

### CUDA Out of Memory (GPU ì„œë²„ ì‚¬ìš© ì‹œ)

#### í•´ê²° ë°©ë²•
```python
# 1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
BATCH_SIZE = 8

# 2. Gradient Accumulation ì‚¬ìš©
GRAD_ACCUM_STEPS = 4  # ì‹¤ì œ ë°°ì¹˜ = 8 Ã— 4 = 32

for step in range(steps):
    loss = model(batch)
    loss = loss / GRAD_ACCUM_STEPS
    loss.backward()

    if (step + 1) % GRAD_ACCUM_STEPS == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()
```

---

## í† í¬ë‚˜ì´ì € ë¬¸ì œ

### í•œêµ­ì–´ê°€ ê¸€ì ë‹¨ìœ„ë¡œ ìª¼ê°œì§

#### ì¦ìƒ
```
"ì•ˆë…•í•˜ì„¸ìš”" â†’ ['ì•ˆ', 'ë…•', 'í•˜', 'ì„¸', 'ìš”']
```

#### ì›ì¸
vocab_sizeê°€ ë„ˆë¬´ ì‘ê±°ë‚˜ í•™ìŠµ ë°ì´í„°ì— í•œêµ­ì–´ê°€ ì ìŒ

#### í•´ê²° ë°©ë²•
```python
# vocab_size ëŠ˜ë¦¬ê¸°
VOCAB_SIZE = 24000  # 16000 â†’ 24000

# ë˜ëŠ” í•œêµ­ì–´ ë°ì´í„° ë” ì¶”ê°€
```

### [UNK] í† í°ì´ ë§ì´ ë‚˜ì˜´

#### ì›ì¸
í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´/í‘œí˜„ì´ ì…ë ¥ë¨

#### í•´ê²° ë°©ë²•
1. í•™ìŠµ ë°ì´í„°ì— ë‹¤ì–‘í•œ í‘œí˜„ ì¶”ê°€
2. í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ê°€ í•™ìŠµ ë„ë©”ì¸ê³¼ ë§ëŠ”ì§€ í™•ì¸
3. vocab_size ëŠ˜ë¦¬ê¸°

### í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨

#### ì¦ìƒ
```
FileNotFoundError: data/tokenizer.json
```

#### í•´ê²° ë°©ë²•
```bash
# í† í¬ë‚˜ì´ì € í•™ìŠµ ë¨¼ì € ì‹¤í–‰
python 02_train_tokenizer.py
```

---

## í•™ìŠµ ê´€ë ¨ ë¬¸ì œ

### Lossê°€ ì¤„ì–´ë“¤ì§€ ì•ŠìŒ

#### ì²´í¬ë¦¬ìŠ¤íŠ¸

1. **í•™ìŠµë¥  í™•ì¸**
```python
# ë„ˆë¬´ ë†’ìœ¼ë©´ ë°œì‚°, ë„ˆë¬´ ë‚®ìœ¼ë©´ ìˆ˜ë ´ ì•ˆ ë¨
LEARNING_RATE = 3e-4  # ê¶Œì¥: 1e-4 ~ 5e-4
```

2. **ë°ì´í„° í™•ì¸**
```python
# ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸
print(f"ë°ì´í„° í¬ê¸°: {len(train_data):,} í† í°")
print(f"ì²« 100ê°œ í† í°: {train_data[:100]}")
```

3. **ëª¨ë¸ í™•ì¸**
```python
# íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
params = sum(p.numel() for p in model.parameters())
print(f"íŒŒë¼ë¯¸í„° ìˆ˜: {params:,}")
```

### Lossê°€ NaN ë˜ëŠ” Inf

#### ì›ì¸
- í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ìŒ
- Gradient Explosion

#### í•´ê²° ë°©ë²•
```python
# 1. í•™ìŠµë¥  ë‚®ì¶”ê¸°
LEARNING_RATE = 1e-4

# 2. Gradient Clipping ì ìš©
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. ë°ì´í„°ì— ì´ìƒí•œ ê°’ ìˆëŠ”ì§€ í™•ì¸
```

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

#### ìµœì í™” ë°©ë²•
```python
# 1. Mixed Precision ì‚¬ìš© (GPU)
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    logits, loss = model(x, targets=y)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

# 2. DataLoader num_workers ëŠ˜ë¦¬ê¸°
dataloader = DataLoader(dataset, num_workers=4, pin_memory=True)

# 3. ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°
with torch.no_grad():  # ì¶”ë¡  ì‹œ
    ...
```

### ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨

#### ì¦ìƒ
```
RuntimeError: Error(s) in loading state_dict for GPT
```

#### ì›ì¸
ì €ì¥í•  ë•Œì™€ ë¡œë“œí•  ë•Œ ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¦„

#### í•´ê²° ë°©ë²•
```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë™ì¼í•œì§€ í™•ì¸
# ì €ì¥ ì‹œ:
# N_LAYER=6, N_HEAD=6, N_EMBD=384

# ë¡œë“œ ì‹œë„ ë™ì¼í•´ì•¼ í•¨:
model = GPT(
    vocab_size=24000,
    block_size=256,
    n_layer=6,      # ë™ì¼í•´ì•¼ í•¨
    n_head=6,       # ë™ì¼í•´ì•¼ í•¨
    n_embd=384,     # ë™ì¼í•´ì•¼ í•¨
    dropout=0.1
)
```

---

## ìƒì„± ê´€ë ¨ ë¬¸ì œ

### ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ë°˜ë³µë¨

#### ì¦ìƒ
```
"ì—°ë½ì²˜ ì—°ë½ì²˜ ì—°ë½ì²˜ ì—°ë½ì²˜..."
```

#### í•´ê²° ë°©ë²•
```python
# 1. Temperature ë†’ì´ê¸°
generate_text(prompt, temperature=1.0)  # 0.7 â†’ 1.0

# 2. Top-K ëŠ˜ë¦¬ê¸°
generate_text(prompt, top_k=80)  # 50 â†’ 80

# 3. Repetition Penalty êµ¬í˜„ (ê³ ê¸‰)
def apply_repetition_penalty(logits, past_tokens, penalty=1.2):
    for token in set(past_tokens):
        logits[token] /= penalty
    return logits
```

### ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ì—‰ëš±í•¨

#### í•´ê²° ë°©ë²•
```python
# 1. Temperature ë‚®ì¶”ê¸°
generate_text(prompt, temperature=0.7)

# 2. Top-K ì¤„ì´ê¸°
generate_text(prompt, top_k=30)

# 3. í”„ë¡¬í”„íŠ¸ í˜•ì‹ í™•ì¸ (í•™ìŠµ í˜•ì‹ê³¼ ë™ì¼í•´ì•¼ í•¨)
prompt = """[QUESTION]
ì§ˆë¬¸ ë‚´ìš©
[/QUESTION]

[ANSWER]
ìš”ì•½:
-"""
```

### [ANSWER] íƒœê·¸ê°€ ë‹«íˆì§€ ì•ŠìŒ

#### ì›ì¸
max_new_tokensê°€ ë¶€ì¡±

#### í•´ê²° ë°©ë²•
```python
generate_text(prompt, max_new_tokens=600)  # 400 â†’ 600
```

### ìƒì„±ì´ ë„ˆë¬´ ëŠë¦¼

#### ìµœì í™” ë°©ë²•
```python
# 1. eval() ëª¨ë“œ í™•ì¸
model.eval()

# 2. torch.no_grad() ì‚¬ìš©
with torch.no_grad():
    output = model.generate(...)

# 3. KV Cache êµ¬í˜„ (ê³ ê¸‰)
# ì´ì „ key, valueë¥¼ ìºì‹œí•˜ì—¬ ì¬ê³„ì‚° ë°©ì§€
```

---

## ë°ì´í„° ê´€ë ¨ ë¬¸ì œ

### í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•¨

#### ì¦ìƒ
- ê³¼ì í•© (train loss â†“, val loss â†‘)
- ìƒì„± í’ˆì§ˆ ì €í•˜

#### í•´ê²° ë°©ë²•
```python
# 1. ë°ì´í„° ì¦ê°•
# - ê°™ì€ ë‚´ìš© ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë³€í˜•
# - ë™ì˜ì–´ ì¹˜í™˜
# - ë¬¸ì¥ ìˆœì„œ ì„ê¸° (ì£¼ì˜í•´ì„œ)

# 2. Dropout ë†’ì´ê¸°
DROPOUT = 0.2  # 0.1 â†’ 0.2

# 3. ì •ê·œí™” ê°•í™”
WEIGHT_DECAY = 0.1  # 0.01 â†’ 0.1
```

### íŠ¹ì • í˜•ì‹ë§Œ ì˜ ìƒì„±ë¨

#### ì›ì¸
í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥

#### í•´ê²° ë°©ë²•
```python
# í•™ìŠµ ë°ì´í„° ê· í˜• ë§ì¶”ê¸°
# - ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì—°ë½ì²˜ ì •ë³´ í¬í•¨
# - ë‹¤ì–‘í•œ ì§ˆë¬¸ í˜•ì‹ í¬í•¨
# - ë‹¤ì–‘í•œ ë‹µë³€ í˜•ì‹ í¬í•¨
```

---

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë°°ì¹˜ í¬ê¸° ìµœì í™”

```python
# MPS ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì ˆ
# M4 ê¸°ì¤€: 16~32 ê¶Œì¥

BATCH_SIZE = 32  # ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìœ¼ë©´
BATCH_SIZE = 16  # ë©”ëª¨ë¦¬ ë¶€ì¡±í•˜ë©´
```

### 2. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)

for step in range(total_steps):
    # ... í•™ìŠµ ...
    scheduler.step()
```

### 3. Early Stopping

```python
best_val_loss = float('inf')
patience = 5
patience_counter = 0

for epoch in range(max_epochs):
    val_loss = validate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        save_checkpoint()
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print("Early stopping!")
        break
```

### 4. íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”©

```python
import numpy as np
import torch

# ë©”ëª¨ë¦¬ ë§¤í•‘ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
data = np.memmap("data/train.bin", dtype=np.uint16, mode='r')

# í•„ìš”í•œ ë¶€ë¶„ë§Œ ë¡œë“œ
batch = torch.from_numpy(data[start:end].astype(np.int64))
```

---

## ë””ë²„ê¹… íŒ

### 1. í…ì„œ í˜•íƒœ í™•ì¸

```python
def debug_shapes(model, x):
    print(f"ì…ë ¥: {x.shape}")

    # ì„ë² ë”© í›„
    tok_emb = model.tok_emb(x)
    print(f"í† í° ì„ë² ë”©: {tok_emb.shape}")

    # ë¸”ë¡ í†µê³¼ í›„
    for i, block in enumerate(model.blocks):
        x = block(x)
        print(f"ë¸”ë¡ {i}: {x.shape}")
```

### 2. Gradient í™•ì¸

```python
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad mean={param.grad.mean():.6f}, std={param.grad.std():.6f}")
```

### 3. Attention ì‹œê°í™”

```python
import matplotlib.pyplot as plt

def visualize_attention(attention_weights, tokens):
    """Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
    plt.figure(figsize=(10, 10))
    plt.imshow(attention_weights, cmap='viridis')
    plt.xticks(range(len(tokens)), tokens, rotation=45)
    plt.yticks(range(len(tokens)), tokens)
    plt.colorbar()
    plt.title("Attention Weights")
    plt.savefig("attention.png")
```

---

## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

### Q: ì–¼ë§ˆë‚˜ í•™ìŠµí•´ì•¼ í•˜ë‚˜ìš”?

**A**: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤.
- 1M í† í°: 5~10 epoch
- 10M í† í°: 2~3 epoch
- 100M+ í† í°: 1 epochë„ ì¶©ë¶„í•  ìˆ˜ ìˆìŒ

Lossê°€ ìˆ˜ë ´í•˜ë©´ í•™ìŠµì„ ë©ˆì¶”ì„¸ìš”.

### Q: GPU ì—†ì´ë„ í•™ìŠµ ê°€ëŠ¥í•œê°€ìš”?

**A**: ê°€ëŠ¥í•˜ì§€ë§Œ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.
- CPU: í•™ìŠµì— ìˆ˜ì¼~ìˆ˜ì£¼ ì†Œìš”
- MPS (Mac): ìˆ˜ì‹œê°„~ìˆ˜ì¼
- CUDA (Nvidia GPU): ìˆ˜ë¶„~ìˆ˜ì‹œê°„

ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.

### Q: ë” í° ëª¨ë¸ì´ í•­ìƒ ì¢‹ì€ê°€ìš”?

**A**: ì•„ë‹™ë‹ˆë‹¤. ë°ì´í„°ê°€ ì ìœ¼ë©´ í° ëª¨ë¸ì€ ê³¼ì í•©ë©ë‹ˆë‹¤.
- ë°ì´í„° < 1M í† í°: ì‘ì€ ëª¨ë¸ ê¶Œì¥
- ë°ì´í„° > 10M í† í°: ë” í° ëª¨ë¸ ì‹œë„ ê°€ëŠ¥

### Q: Temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´?

**A**: Greedy Decodingì´ ë©ë‹ˆë‹¤.
- í•­ìƒ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ
- ê²°ì •ì  ì¶œë ¥ (ê°™ì€ ì…ë ¥ â†’ ê°™ì€ ì¶œë ¥)
- ë‹¤ì–‘ì„± ì—†ìŒ

### Q: ëª¨ë¸ì„ ì–´ë–»ê²Œ ë°°í¬í•˜ë‚˜ìš”?

**A**: ì—¬ëŸ¬ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.
```python
# 1. ì§ì ‘ ë¡œë“œ
model = load_checkpoint("ckpt.pt")

# 2. ONNX ë³€í™˜
torch.onnx.export(model, dummy_input, "model.onnx")

# 3. TorchScript
scripted = torch.jit.script(model)
scripted.save("model.pt")
```

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/)
- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)

### ë…¼ë¬¸
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer ì›ë³¸ ë…¼ë¬¸
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 ë…¼ë¬¸

### êµ¬í˜„ ì°¸ê³ 
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathyì˜ ë¯¸ë‹ˆ GPT êµ¬í˜„
- [minGPT](https://github.com/karpathy/minGPT) - ë” ê°„ë‹¨í•œ GPT êµ¬í˜„

---

## ë‹¤ìŒ ë‹¨ê³„

ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰ CAI í•™ìŠµ ë¬¸ì„œë¥¼ ëª¨ë‘ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.

### ì¶”ì²œ í•™ìŠµ ìˆœì„œ

1. [00-overview.md](00-overview.md) - í”„ë¡œì íŠ¸ ê°œìš”
2. [01-environment-setup.md](01-environment-setup.md) - í™˜ê²½ ì„¤ì •
3. [02-project-structure.md](02-project-structure.md) - í´ë” êµ¬ì¡°
4. [03-data-preparation.md](03-data-preparation.md) - ë°ì´í„° ì¤€ë¹„
5. [04-tokenizer.md](04-tokenizer.md) - í† í¬ë‚˜ì´ì € í•™ìŠµ
6. [05-model-architecture.md](05-model-architecture.md) - ëª¨ë¸ êµ¬ì¡°
7. [06-training.md](06-training.md) - ëª¨ë¸ í•™ìŠµ
8. [07-generation.md](07-generation.md) - í…ìŠ¤íŠ¸ ìƒì„±
9. [08-concepts.md](08-concepts.md) - í•µì‹¬ ê°œë…
10. [09-tips.md](09-tips.md) - íŒê³¼ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (í˜„ì¬ ë¬¸ì„œ)

### ì‹¤ìŠµ ìˆœì„œ

```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜ (uv ì‚¬ìš©)
uv add torch tokenizers tqdm numpy

# 2. ë°ì´í„° ì¤€ë¹„
uv run python scripts/prepare_samples.py

# 3. í† í¬ë‚˜ì´ì € í•™ìŠµ
uv run python scripts/train_tokenizer.py

# 4. ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì…‹ ìƒì„±
uv run python scripts/build_bin_dataset.py

# 5. ëª¨ë¸ í•™ìŠµ (MPS fallback í•„ìš”)
PYTORCH_ENABLE_MPS_FALLBACK=1 uv run python scripts/train_gpt.py

# 6. í…ìŠ¤íŠ¸ ìƒì„±
uv run python scripts/generate.py
```

ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ Contact AIë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!
